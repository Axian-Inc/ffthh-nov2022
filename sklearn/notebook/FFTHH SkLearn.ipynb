{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bfefb7",
   "metadata": {},
   "source": [
    "# FFTHH November - Machine Learning Task\n",
    "\n",
    "This month's FFTHH is going to be a basic machine-learning task, using some bread-and-butter tools.\n",
    "\n",
    "<details>\n",
    "\n",
    "\n",
    "<summary>What Are We Going to Do?</summary>\n",
    "\n",
    "We're going to use some basic tools to do a basic machine learning task: reading digitized handwritten characters and classifying them as machine-readable digits.  \n",
    "\n",
    "To do this, we'll use a well-known, publicly available dataset: the [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) for handwritten digits handwritten digits:\n",
    "\n",
    "Our task is to take the variable, noisy input from digits written by humans and reduce them to information easily understood by machines.  \n",
    "![image](480px-MnistExamples.png \"mnist-samples\")\n",
    "\n",
    "# &#x21D3;\n",
    "\n",
    "[0, 1, 2, 3, 4, 5, etc... ]\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "\n",
    "<summary>Procedure</summary>\n",
    "\n",
    "We'll be going through some of the steps that we do for machine-learning tasks, more generally. This means we'll be doing: \n",
    "\n",
    "- dataset building\n",
    "- exploratory data analysis\n",
    "- feature engineering\n",
    "- model selection\n",
    "- model training\n",
    "- model validation\n",
    "- hyperparameter tuning\n",
    "\n",
    "We'll go through what each of these things means in more detail below.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>What Tools are We Going to Use?</summary>\n",
    "\n",
    "We'll be using the following libraries/frameworks:\n",
    "\n",
    "- [Jupyter](https://jupyter.org/) notebooks running in an [Anaconda](https://www.anaconda.com/) environment for running our code in an interactive Python environment, with the needed data-science dependencies already pre-loaded\n",
    "- [NumPy](https://numpy.org/) for manipulating matrices/arrays efficiently\n",
    "- [Pandas](https://pandas.pydata.org/) for manipulating our datasets and doing feature engineering\n",
    "- [Scikit-Learn](https://scikit-learn.org/stable/) for doing our main machine-learning heavy-lifting tasks\n",
    "- [Matplotlib](https://matplotlib.org/) and [Graphviz](https://graphviz.org/) for doing visualization and charting\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import model_selection as model_selection\n",
    "from sklearn.tree import export_graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV, cross_val_score\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f764c6",
   "metadata": {},
   "source": [
    "# Get a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d8bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this statement will use scikit-learn's built-in API for fetching datasets\n",
    "# https://www.openml.org/search?type=data&sort=runs&id=554&status=active\n",
    "dataset = fetch_openml(\"mnist_784\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first take a look around\n",
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13de5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7bead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, sklearn returns datasets as tuples of numpy arrays. \n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using Pandas for some of our feature engineering work, so we'll be transforming the datasets into Pandas dataframes.\n",
    "# this will get our dataset as a single pandas dataframe. \n",
    "# this makes things like exploratory data analysis easy.\n",
    "\n",
    "df_X, df_y = pd.DataFrame(X), pd.DataFrame(y)\n",
    "df = df_X.join(df_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe144a5",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "<details>\n",
    "<summary>What is EDA?</summary>\n",
    "\n",
    "The point of EDA is \n",
    "- To understand our dataset, \n",
    "- To inform our downstream tasks\n",
    "\n",
    "Since we're doing an OCR Task, we'll want to know  what our dataset looks like, with that goal in mind. \n",
    "\n",
    "It helps us recognize problems and necessary data manipulation steps early.  We don't want to spend lots of money and time on model training or manual labelling tasks only to have to redo that work later. EDA can help us reduce the cycle time for our work.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>What do we know already, before doing EDA?</summary>\n",
    "\n",
    "First, it helps to consult the [documentation](https://www.openml.org/search?type=data&sort=runs&id=554&status=active). The documentation tells us \n",
    "- that we have a dataset with 784 features\n",
    "- that it represents 28 x 28 pixel greyscale images, with the images centered in the frame, with pixel darkness ranging from 0 (white) to 255 (black).\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>What kinds of things do we learn by doing EDA?</summary>\n",
    "\n",
    "for example:\n",
    "- Variable Distributions\n",
    "- Relationships between predictors\n",
    "- missing values \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abe107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out what the columns mean\n",
    "# Visualization helps\n",
    "\n",
    "digit_idx = 5000\n",
    "\n",
    "# this gets us a row of pixel data\n",
    "digit = df_X.iloc[digit_idx]\n",
    "\n",
    "# reshape the data to be a matrix, and then chart it with matplotlib\n",
    "digit_img = digit.to_numpy().reshape(28, 28)\n",
    "plt.imshow(digit_img, cmap=matplotlib.cm.binary, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what the label for the features is\n",
    "df_y.iloc[digit_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d627bb5",
   "metadata": {},
   "source": [
    "### Now that we understand how our data are organized, let's perform some basic data hygiene tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e152666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get descriptive stats on subsets of pixels.\n",
    "# pandas has a describe() method\n",
    "\n",
    "df[[ \n",
    "    'pixel1', 'pixel2', 'pixel3', \n",
    "    'pixel391', 'pixel392', 'pixel393', \n",
    "    'pixel782', 'pixel783', 'pixel784']] \\\n",
    ".describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b8e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column=['pixel1', 'pixel2', 'pixel3', 'pixel391', 'pixel392', 'pixel393', 'pixel782', 'pixel783', 'pixel784'],\n",
    "    bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c84c983",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>What has our minimal EDA shown us?</summary>\n",
    "\n",
    "- The first pixels and the last pixels in the array are never (or almost never) filled in.\n",
    "- By comparison, we see that the average number for the middle pixels, are, on average, darker. But still, they are far more likely to be blank.\n",
    "- Even though theoretically, cells are bound between 0 and 255, the actual values are more varied\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>What should we do with this information?</summary>\n",
    "\n",
    "Depending on which model we select, we may have to 'massage' these features so that they meet certain key assumptions of the model. \n",
    "\"Massaging\" our columns to make model performance better, is called \"Feature Engineering\"\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a292f",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "- Missing Values can be a show-stopper\n",
    "- It looks like we didn't have any missing values this time. But what if we had? \n",
    "- *Imputation* means filling in missing values. There are a few different ways to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b98786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual check for missing values\n",
    "\n",
    "df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a non-zero value, so we can artificially remove it\n",
    "df.query('pixel392 > 0')['pixel392']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artificially remove a value\n",
    "\n",
    "df.loc[22617, 'pixel392'] = np.nan\n",
    "df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc95a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do imputation\n",
    "\n",
    "# scalar imputation using forward- or back-fill *might* be a decent choice here, \n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna\n",
    "\n",
    "df['pixel392'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# check what was imputed\n",
    "print(df.loc[22617, 'pixel392'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fffc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It just filled in zero because that's what the previous observation had for that pixel. Which, is really kind of arbitrary. \n",
    "# And, it's only using ONE piece of information to make the decision. \n",
    "# a better choice might be an imputer from SkLearn:\n",
    "# https://scikit-learn.org/stable/modules/impute.html\n",
    "df.loc[22617, 'pixel392'] = np.nan\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = pd.DataFrame(imputer.transform(df))\n",
    "df_imputed.columns = df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac04d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed[[ \n",
    "    'pixel1', 'pixel2', 'pixel3', \n",
    "    'pixel391', 'pixel392', 'pixel393', \n",
    "    'pixel782', 'pixel783', 'pixel784']] \\\n",
    ".describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list any missings - expect to be empty\n",
    "print(df_imputed.columns[df_imputed.isna().any()].tolist())\n",
    "\n",
    "# list the value for the imputed missing datum\n",
    "print(df_imputed.loc[22617, 'pixel392'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ff43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's still off but it's better than the first one.\n",
    "# there are better methods of imputation that the SimpleImputer, but for our purposes it's probably good enough. Let's move on.\n",
    "df = df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8276a83",
   "metadata": {},
   "source": [
    "### Was this imputation good? \n",
    "\n",
    "Maybe we could do better with an [Iterative Imputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30935d",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "<details>\n",
    "\n",
    "<summary>What is Scaling?</summary>\n",
    "\n",
    "\n",
    "Scaling means changing the value of each data point to be within a specific range. \n",
    "- Sometimes it is a linear transformation, like multiplying each data value in a column by a constant. \n",
    "- Sometimes it is not linear, like applying an exponential function or using a mathematical distribution to reassign a value.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Why scaling?</summary>\n",
    "\n",
    "\n",
    "- Some models have a prerequisite that each column is bounded by values that are the same, or similar\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>What kinds of scaling are there?</summary>\n",
    "\n",
    "\n",
    "- Min-max scaling\n",
    "- Standard scaling\n",
    "- other variations (e.g., Adaptive Scaling)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing scaling on the features:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# scaler fit_transform calls fit() and transform() all in one, for convenience\n",
    "for x in df.columns:\n",
    "    if x.startswith('pixel'):\n",
    "        df[f'{x}_scaled'] = scaler.fit_transform(df[x].to_numpy().reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020277a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that our scaling worked\n",
    "\n",
    "df[[ \n",
    "    'pixel1_scaled', 'pixel2_scaled', 'pixel3_scaled', \n",
    "    'pixel391_scaled', 'pixel392_scaled', 'pixel393_scaled', \n",
    "    'pixel782_scaled', 'pixel783_scaled', 'pixel784_scaled']] \\\n",
    ".describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a20f13",
   "metadata": {},
   "source": [
    "# Model Selection after Feature Engineering\n",
    "\n",
    "<details>\n",
    "<summary>What is Model Selection?</summary>\n",
    "\n",
    "- It's really just selecting a model.\n",
    "Realistically, Feature Engineering and Model Selection are often iterative processes. After Exploratory Data Analysis, we might try an initial round of feature engineering, followed by exploring some prototype models.  If those initial models don't prove to have good performance, we might try again by crafting new features or finding out how to improve the validity of the ones we have.  \n",
    "\n",
    "For this exercise, though, we'll assume that we've done all of the feature engineering that we need to have done, and we can move on to selecting models.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>How do we do it?</summary>\n",
    "\n",
    "\n",
    "Short answer is that depends; it can be kind of hard.\n",
    "\n",
    "Maybe the most important part of model selection is knowing enough about:\n",
    "- The problem you're trying to solve\n",
    "- The nature of your dataset\n",
    "\n",
    "Assumptions (read 'requirements') that the dataset needs to meet, in order for the model to work.\n",
    "\n",
    "In real practice, there's often some trial-and-error.  It is common to try multiple models and pick the one with the best results.\n",
    "For example, for our problem (classifying numerals from digitized handwritten digits),  we might try ...: \n",
    "- Artificial neural nets\n",
    "- Decision Trees\n",
    "- Logistic regression\n",
    "\n",
    "... We \n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>How do we make the decision?</summary>\n",
    "\n",
    "1. The the model's appropriateness for the problem\n",
    "- Categorization model vs Time-series model? \n",
    "- Regression Model\n",
    "- Supervised/Unsupervised learning\n",
    "\n",
    "2. The model's appropriateness for the data (does the model meet assumptions for interpretability of the results)\n",
    "- Independence of observations from each other\n",
    "- Independence of features from each other\n",
    "- Normally distributed predictors (histogram of the predictor roughly follows Gaussian distribution)\n",
    "- Homoskedasticity (variability in predictors needs to be unrelated to central tendency)\n",
    "\n",
    "3. Operational constraints\n",
    "- Will it be efficient to train?\n",
    "- Is any special human effort involved (i.e., coding for training data)? \n",
    "- Will it be efficient to deploy and maintain over time?\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>What model did we pick?</summary>\n",
    "\n",
    "For this exercise, we'll just pick a Random Forests Classifier\n",
    "\n",
    "- It's easy to understand\n",
    "- It has lax assumptions/requirements\n",
    "- It shows us how ensembles work\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed320eb",
   "metadata": {},
   "source": [
    "# Training the Classifier\n",
    "\n",
    "## Example Model: Random Forest Classifier (RFC)\n",
    "\n",
    "<details>\n",
    "<summary>What are Random Forests?</summary>\n",
    "\n",
    "\n",
    "Random Forests are an Ensemble classifier made up of Decision Trees\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>So, then what are Decision Trees?</summary>\n",
    "\n",
    "A Decision Trees is a classifier that works by branching conditional probabilities. Simply put, it is a process that takes input and tries to find the threshold points in different features (given prior choices) that can optimally sort the input into the target categories.\n",
    "\n",
    "There is more than one possible target metric, but the default value that is used by scikit-learn for optimization is called the _Gini Impurity_.  Lower Gini impurity is an indicator of a the more accurate model, with a value of 0 representing a perfect model.\n",
    "\n",
    "see: [gini impurity explained](https://towardsdatascience.com/gini-impurity-measure-dbd3878ead33)\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Decision Trees Visualized</summary>\n",
    "\n",
    "Example Decision Tree:\n",
    "![image](tree_short.png \"Example Decision Tree\")\n",
    "\n",
    "\n",
    "A Random Forest Classifier is simply a collection of Decision Trees. It is what is known as an 'ensemble' method, so called because the model is a collection of smaller models rather than being a single model. The collection of trees act as \"voters\" and the output class that has the most votes is the one chosen by the model.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e971b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset by\n",
    "# train versus test \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "df_train, df_test = model_selection.train_test_split(df)\n",
    "\n",
    "# split by \n",
    "# Features versus labels\n",
    "# 1 pandas dataframe -> 4 numpy array\n",
    "x_columns = list([c for c in df.columns if c.startswith('pixel') and c.endswith('scaled')])\n",
    "\n",
    "def split_xy(df_whole, x_columnnames):\n",
    "    x_df = df_whole[x_columnnames]\n",
    "    y_df = df_whole['class']\n",
    "\n",
    "    return x_df, y_df\n",
    "\n",
    "df_train_x, df_train_y = split_xy(df_train, x_columns)\n",
    "df_test_x, df_test_y = split_xy(df_test, x_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef49b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the shape of our data; it should be something that makes sense\n",
    "print(df_train_x.shape)\n",
    "print(df_train_y.shape) \n",
    "print(df_test_x.shape)\n",
    "print(df_test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd600ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# in Scikit-learn, hyperparameters are passed into the classifier's constructor\n",
    "rfclassifier = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    n_jobs=4)\n",
    "\n",
    "rfclassifier.fit(X=df_train_x,y=df_train_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016dcbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This RFC has hyperparameters that limit the depth of the tree and the number of samples per leaf node\n",
    "stubbyrfc = RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=10,\n",
    "    n_jobs=4)\n",
    "\n",
    "stubbyrfc.fit(X=df_train_x,y=df_train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OPTIONAL STEP\n",
    "# get a visualization of one of our trees.\n",
    "# we can use Graphviz, which is an open source visualization tool \n",
    "# https://graphviz.org/download/\n",
    "\n",
    "sample_tree = rfclassifier.estimators_[42]\n",
    "export_graphviz(sample_tree,\n",
    "    out_file='tree.dot',\n",
    "    feature_names=x_columns,    \n",
    "    rounded=True,\n",
    "    proportion=False,\n",
    "    precision=2,\n",
    "    filled=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stubby_sample = stubbyrfc.estimators_[4]\n",
    "export_graphviz(stubby_sample,\n",
    "    out_file='tree_short.dot',\n",
    "    feature_names=x_columns,    \n",
    "    rounded=True,\n",
    "    proportion=False,\n",
    "    precision=2,\n",
    "    filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743595b",
   "metadata": {},
   "source": [
    "With the Graphviz tool we, can take a look at the topology of one of the decision trees that we built.\n",
    "\n",
    "we need to do a little processing outside of this Jupyter notebook to invoke Graphviz to turn the `.dot` file into a png:\n",
    "\n",
    "```\n",
    "dot -Tpng .\\tree.dot -o .\\tree.png\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "One thing we notice right away is that it's BIG.  Here is what the whole thing looks like:\n",
    "\n",
    "![image](tree.png \"The Whole Tree\")\n",
    "\n",
    "That's hard to see. Here's a closer-up sample of the image:\n",
    "\n",
    "![image](tree_sample.png \"Sample of One Tree\")\n",
    "\n",
    "\n",
    "One thing that this image shows us is that we may have been lenient with the depth of our our trees, or how many branches we've allowed to be created.  \n",
    "\n",
    "This is not necessarily bad. Models with lots of features and output classes might need to have more depth in order to build a well-performing set of trees.  However, a tree with unbounded or overly lenient hyperparameters that allow it to build out deeper and broader trees, this might be a cause of overfitting (if overfitting is found).  At any rate, We haven't yet established whether our model is indeed overfitting, so let's not get ahead of ourselves.  We will see below about what effect our model hyperparameters have on the shape of each individual tree, and on how the multiple trees behave together in the ensemble classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02f035",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance, Hyperparameter Tuning\n",
    "\n",
    "Simple Evaluation\n",
    "Cross-Validation\n",
    "Tuning HPs\n",
    "- Different HPs for different models.\n",
    "- [Hyperparameters for a Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) include\n",
    "    - Maximum depth\n",
    "    - Minimum change in target metric before quitting\n",
    "    - Minimum samples per node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model's initial prediction.\n",
    "# Here we use our holdout sample to test whether we've done overfitting.\n",
    "# If the model's performance on the holdout\n",
    "\n",
    "pred_y = rfclassifier.predict(X=df_test_x)\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=df_test_y, y_pred=pred_y)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c05106",
   "metadata": {},
   "outputs": [],
   "source": [
    "stb_pred_y = stubbyrfc.predict(X=df_test_x)\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=df_test_y, y_pred=stb_pred_y)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45a047",
   "metadata": {},
   "source": [
    "### What we just did above with these two models is an example of **Hyperparameter Tuning**\n",
    "\n",
    "<details>\n",
    "<summary>What does the comparison between these two models tell us?</summary>\n",
    "\n",
    "- The top one was better\n",
    "- But why? we don't always know. In general, HP tuning provides a balance between:\n",
    "    - Addressing overfitting vs\n",
    "    - Providing sufficient complexity to handle the problem\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Can we Automate Hyperparameter tuning?</summary>\n",
    "\n",
    "[Yes!](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV)\n",
    "\n",
    "Scikit-Learn comes with tools to explore different samplings of hyperparameters to help find optimal combinations of values.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated HP Tuning. This Takes about 11 minutes\n",
    "\n",
    "# get an RFC with no HPs specified\n",
    "# n_jobs and random_state don't affect the model characteristics\n",
    "raw_rfc = RandomForestClassifier(n_jobs=4, random_state=42)\n",
    "param_distributions = {\n",
    "    'max_depth': [3, 10],\n",
    "    'min_samples_split': randint(2, 11)\n",
    "}\n",
    "\n",
    "randomsearch_validator = HalvingRandomSearchCV(raw_rfc, \n",
    "    param_distributions, \n",
    "    random_state=42).fit(df_train_x, df_train_y)\n",
    "\n",
    "randomsearch_validator.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10dc70c",
   "metadata": {},
   "source": [
    "Pasting the output of the above so that we don't have to run it for a long time:\n",
    "\n",
    "![Image](HPTune_output.png \"hp tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a3591",
   "metadata": {},
   "source": [
    "## More Cross-Validation\n",
    "\n",
    "Doing a single comparison with a holdout sample is good, but a good, generalizable model should be able to do that with more than just one split between estimation and test samples.  That is where cross-validation helps. It replicates the estimation/sample split with multiple subsamples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html?highlight=cross_val_score#sklearn.model_selection.cross_val_score\n",
    "cross_val_score(rfclassifier, \n",
    "    df_train_x, \n",
    "    df_train_y, \n",
    "    cv=3, \n",
    "    scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb867b1",
   "metadata": {},
   "source": [
    "### Interpretation of Model Performance\n",
    "\n",
    "So, we've gotten our metrics back about our model's performance. At first blush, it seems pretty good. But how does it translate to our business needs?  There are different metrics we can use to evaluate a model. Here are a few:\n",
    "\n",
    "<details style=\"background-color:white;color:black;\">\n",
    "<summary>Accuracy</summary>\n",
    "\n",
    "This is conceptually familiar, but what does it mean? \n",
    "\n",
    "![Image](accuracy.svg \"accuracy\")\n",
    "</details>\n",
    "\n",
    "<details style=\"background-color:white;color:black;\">\n",
    "<summary>Precision & Recall</summary>\n",
    "\n",
    "\n",
    "How likely are we to correctly detect a true positive?\n",
    "\n",
    "How likely are we to correctly reject a \"false negative?\"\n",
    "![Image](recall.svg \"recall\")\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea853ed",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ROC Plotting</summary>\n",
    "\n",
    "How can we understand metrics like precision and recall as they pertain to our dataset?\n",
    "\n",
    "Looking at plots of Receiver Operating Characteristics (ROC) can help\n",
    "\n",
    "![Image](roccurves.png \"example ROC curve\")\n",
    "source: By BOR at the English-language Wikipedia, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=10714489\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>What Does an ROC Plot Tell Us?</summary>\n",
    "\n",
    "- Plots True Positives against False Positives\n",
    "- Compares these 2 metrics at various model \"thresholds\"\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this takes about 45 sec\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# one-hot encoding for output vars\n",
    "binary_y = label_binarize(df_train_y, classes=[0,1,2,3,4,5,6,7,8,9])\n",
    "binary_y_test = label_binarize(df_test_y, classes=[0,1,2,3,4,5,6,7,8,9])\n",
    "n_classes = binary_y.shape[1]\n",
    "\n",
    "onevrest = OneVsRestClassifier(rfclassifier)\n",
    "y_score = onevrest.fit(df_train_x, df_train_y).predict_proba(df_test_x)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(binary_y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "plt.figure(figsize=(8,8), dpi=150)\n",
    "lw=2\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\", \"forestgreen\", \"firebrick\", \"coral\", \"rebeccapurple\", \"lightsalmon\", \"darkslategray\", \"cadetblue\"])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    print(f\"i {i}, color {color}\")    \n",
    "    plt.plot(\n",
    "        fpr[i],\n",
    "        tpr[i],\n",
    "        color=color,\n",
    "        lw=lw,\n",
    "        label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i, roc_auc[i]),\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Some extension of Receiver operating characteristic to multiclass\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ababb8b",
   "metadata": {},
   "source": [
    "Using the `predict_proba` metric as a tool, our classifier looks pretty good, for something that we just came up with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ccf819",
   "metadata": {},
   "source": [
    "## Sources/References/Further Reading\n",
    "\n",
    "[Aurelien Geron's ML Book from O'Reilly](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "\n",
    "[Kaggle's Public Datasets](https://www.kaggle.com/datasets)\n",
    "\n",
    "[Portland Data Science Meetup Group](https://www.meetup.com/Portland-Data-Science-Group/?_cookie-check=gA3HOsispN8xbOgE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
